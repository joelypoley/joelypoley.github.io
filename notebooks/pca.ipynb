{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis, pictures, code and proofs\n",
    "\n",
    "Principal component analysis is used to reduce the the number of dimensions needed to represent your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_data(angle, x_len, y_len, x_offset, y_offset, num_examples):\n",
    "    theta = np.radians(angle)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c,-s), (s, c)))\n",
    "    D = np.array([[x_len, 0], [0, y_len]])\n",
    "\n",
    "    mean = [x_offset, y_offset]\n",
    "    cov = np.linalg.inv(R) @ D @ R  # diagonal covariance\n",
    "    print(cov)\n",
    "\n",
    "    x, y = np.random.multivariate_normal(mean, cov, num_examples).T\n",
    "    X = np.array(list(zip(x, y))).T\n",
    "    return X\n",
    "\n",
    "num_examples = 100\n",
    "X = generate_data(45, 0.03, 1, 3, 1, num_examples)\n",
    "print(X.shape)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "x_line = np.linspace(1, 5, 100)\n",
    "y_line = x_line - 2\n",
    "ax.plot(X[0,:], X[1,:],'bx', x_line, y_line, 'r', linestyle='None')\n",
    "ax.axis('equal')\n",
    "ax.axhline(0, color='gray')\n",
    "ax.axvline(0, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above is two dimensional, but it is \"almost\" one dimensional in the sense that every point is close to a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.linspace(1, 5, 100)\n",
    "y_line = x_line - 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[0,:], X[1,:],'bx', x_line, y_line, 'r')\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis can help us understand the data better. Given the list of 2d points, $x_1, x_2, \\dots , x_n \\in \\mathbb{R}^2$ we first center the data by calculating the mean $\\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and replacing each $x_i$ with $x_i - \\overline{x}$. Now the data looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X - np.reshape(np.mean(X, axis=1), [2, 1])\n",
    "x_line = np.linspace(-2, 2, 100)\n",
    "y_line = x_line\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[0,:], X[1,:],'bx', x_line, y_line, 'r', linestyle='None')\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the data in a matrix.\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "| & | &  & | \\\\\n",
    "x_1 & x_2 &\\cdots & x_n \\\\\n",
    "| & | &  & |\\end{pmatrix}.\n",
    "$$\n",
    "And calculate the eigenvectors and eigenvalues of the *covariance matrix* $\\frac{1}{n-1}XX^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = (1/(num_examples - 1)) * X @ X.T\n",
    "e_vals, e_vecs = np.linalg.eig(cov)\n",
    "lambda_1, lambda_2 = np.sqrt(e_vals)\n",
    "v_1, v_2 = e_vecs[0], e_vecs[1]\n",
    "print('e vals = ', e_vals)\n",
    "print('e_vecs = ', e_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the eigenvectors and eigenvalues of the covariance matrix tell us? Well lets plot the eigenvectors with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[0,:], X[1,:],'bx')\n",
    "ax.arrow(0, 0, e_vecs[0, 0], e_vecs[1, 0], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.arrow(0, 0, e_vecs[0, 1], e_vecs[1, 1], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors tell us the *direction* of the data. The first eignevector has the same slope as the data and the second eigenvector is perpendicular to the first. Now let's scale each of the eigenvectors by two time the square root of the eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[0,:], X[1,:],'bx')\n",
    "ax.arrow(0, 0, 2 * lambda_1 * e_vecs[0, 0], 2 * lambda_1 * e_vecs[1, 0], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.arrow(0, 0, 2 * lambda_2 * e_vecs[0, 1], 2 * lambda_2 * e_vecs[1, 1], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And draw an ellipse around the data eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[0,:], X[1,:],'bx')\n",
    "ax.arrow(0, 0, 2 * lambda_1 * e_vecs[0, 0], 2 * lambda_1 * e_vecs[1, 0], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.arrow(0, 0, 2 * lambda_2 * e_vecs[0, 1], 2 * lambda_2 * e_vecs[1, 1], head_width=0.1, length_includes_head=True, color='red')\n",
    "e = Ellipse(xy=(0, 0), width=4*lambda_2, height=4*lambda_1, angle=45)\n",
    "ax.add_artist(e)\n",
    "e.set_clip_box(ax.bbox)\n",
    "e.set_alpha(0.15)\n",
    "e.set_facecolor([1, 0, 0])\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues tell us how spread out the data is in the direction of that particular eigenvalue. Thus we can reduce the dimension of the data by projecting onto the line given by the largest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (1 / np.sqrt(2)) * np.array([[1], [1]])\n",
    "Pu = u @ u.T\n",
    "X_projected = Pu @ X\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_projected[0,:], X_projected[1,:],'bx')\n",
    "ax.arrow(0, 0, 2 * lambda_1 * e_vecs[0, 0], 2 * lambda_1 * e_vecs[1, 0], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.arrow(0, 0, 2 * lambda_2 * e_vecs[0, 1], 2 * lambda_2 * e_vecs[1, 1], head_width=0.1, length_includes_head=True, color='red')\n",
    "ax.axis('equal')\n",
    "ax.axhline(color='gray')\n",
    "ax.axvline(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now one dimensional since it fits on a single line and each point is has not moved too far from its original spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two dimensions this is the same as projecitng onto the line of best fit, but this technique generalizes. If your data is $n$-dimensional then PCA lets you find the best $m$-dimensional subspace to project the data down onto; you just project your data onto the subspace spanned by the $m$ eigenvectors with the largest eigenvalues. If $m << n$ this can compress your data a lot, and PCA guarantees that this $m$ dimensional subspace is optimal in the sense that it minimizes the mean squared error between the original data points and the projected data points.\n",
    "\n",
    "The data in the plots above was generated using a random number generator. Let's try PCA on a real dataset.\n",
    "\n",
    "We will use the mnist dataset, which is a collection of grayscale, 28x28 images of hand written digits. To simplify the analysis we will only look at zeros and ones in the dataset. Below is an example of the images from mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "ones = []\n",
    "zeros = []\n",
    "with open('train.csv', 'r') as csvfile:\n",
    "    next(csvfile)\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        label = int(row[0])\n",
    "        image = [int(x) for x in row[1:]]\n",
    "        if label == 0:\n",
    "            zeros.append(image)\n",
    "        elif label == 1:\n",
    "            ones.append(image)\n",
    "\n",
    "zeros = np.array(zeros).T\n",
    "ones = np.array(ones).T\n",
    "print(zeros.shape)\n",
    "print(ones.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(1, 2+1):\n",
    "    fig.add_subplot(2, 2, i)\n",
    "    plt.imshow(np.reshape(zeros[:,i], [28, 28]), cmap='gray')\n",
    "for i in range(2+1, 4+1):\n",
    "    fig.add_subplot(2, 2, i)\n",
    "    plt.imshow(np.reshape(ones[:,i], [28, 28]), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now:\n",
    " * Flatten each image into a $784 = 28\\times 28$ vector.\n",
    " * Use PCA to project each 784 dimensional vector to a vector in $\\mathbb{R}^2$.\n",
    " * Plot the 2 dimensional vectors, with images of '0' in red and images of '1' in blue.\n",
    "\n",
    "The result looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "zeros_and_ones = np.concatenate((zeros, ones), axis=1)\n",
    "print(zeros_and_ones.shape)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(zeros_and_ones.T)\n",
    "print(principalComponents[:4,:])\n",
    "print(principalComponents[:, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "ax.plot(principalComponents[:4132, 0], principalComponents[:4132, 1],'bx')\n",
    "ax.plot(principalComponents[4132:, 0], principalComponents[4132:, 1],'rx')\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(zeros_and_ones.T)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(principalComponents[:4132, 0], principalComponents[:4132, 1], principalComponents[:4132, 2], c='b', marker='x')\n",
    "ax.scatter(principalComponents[4132:, 0], principalComponents[4132:, 1], principalComponents[4132:, 2], c='r', marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30\n",
    "pca_large = PCA(n_components=n_components)\n",
    "principalComponents = pca_large.fit_transform(zeros_and_ones.T)\n",
    "# print(np.arange(1, 785))\n",
    "# print(pca_large.explained_variance_)\n",
    "fig, ax = plt.subplots()\n",
    "#ax.set_aspect('equal')\n",
    "#ax.grid(True, which='both')\n",
    "ax.bar(np.arange(n_components), pca_large.explained_variance_ , 0.3)\n",
    "#ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
